{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816ae564",
   "metadata": {},
   "source": [
    "# ç†è§£ output_attentions çš„ä½œç”¨\n",
    "\n",
    "## ä»€ä¹ˆæ˜¯ Attentionï¼ˆæ³¨æ„åŠ›ï¼‰ï¼Ÿ\n",
    "æƒ³è±¡ä½ åœ¨è¯»ä¸€å¥è¯ï¼š\"æˆ‘çˆ±åƒè‹¹æœ\"ã€‚å½“æ¨¡å‹å¤„ç†\"è‹¹æœ\"è¿™ä¸ªè¯æ—¶ï¼Œå®ƒä¼šï¼š\n",
    "- å…³æ³¨\"æˆ‘\"ï¼ˆè°åœ¨åƒï¼Ÿï¼‰\n",
    "- å…³æ³¨\"çˆ±åƒ\"ï¼ˆä»€ä¹ˆåŠ¨ä½œï¼Ÿï¼‰\n",
    "- å…³æ³¨\"è‹¹æœ\"æœ¬èº«\n",
    "\n",
    "è¿™ç§\"å…³æ³¨ç¨‹åº¦\"å°±æ˜¯ **attention scoresï¼ˆæ³¨æ„åŠ›åˆ†æ•°ï¼‰**\n",
    "\n",
    "## output_attentions çš„ä½œç”¨\n",
    "- `output_attentions = False`ï¼ˆé»˜è®¤ï¼‰ï¼šæ¨¡å‹åªç»™ä½ æœ€ç»ˆç»“æœï¼Œä¸å‘Šè¯‰ä½ å®ƒæ˜¯æ€ä¹ˆ\"å…³æ³¨\"å„ä¸ªè¯çš„\n",
    "- `output_attentions = True`ï¼šæ¨¡å‹ä¼šé¢å¤–è¿”å›æ¯ä¸€å±‚ã€æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œè®©ä½ çœ‹åˆ°æ¨¡å‹çš„\"æ€è€ƒè¿‡ç¨‹\"\n",
    "\n",
    "ä¸‹é¢ç”¨ä»£ç æ¼”ç¤ºä¸¤è€…çš„åŒºåˆ«ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e79286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ç¤ºä¾‹1ï¼šoutput_attentions = Falseï¼ˆé»˜è®¤æƒ…å†µï¼‰\n",
      "============================================================\n",
      "è¿”å›çš„å†…å®¹æœ‰è¿™äº›å±æ€§ï¼šodict_keys(['logits', 'past_key_values'])\n",
      "\n",
      "outputs.logits çš„å½¢çŠ¶: torch.Size([1, 3, 50257])\n",
      "outputs.attentions æ˜¯å¦ä¸º None: True\n",
      "\n",
      "ç»“è®ºï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹åªè¿”å› logitsï¼ˆé¢„æµ‹ç»“æœï¼‰ï¼Œä¸è¿”å› attentions\n",
      "\n",
      "============================================================\n",
      "ç¤ºä¾‹2ï¼šoutput_attentions = True\n",
      "============================================================\n",
      "è¿”å›çš„å†…å®¹æœ‰è¿™äº›å±æ€§ï¼šodict_keys(['logits', 'past_key_values'])\n",
      "\n",
      "outputs.logits çš„å½¢çŠ¶: torch.Size([1, 3, 50257])\n",
      "outputs.attentions æ˜¯å¦ä¸º None: True\n",
      "\n",
      "ç»“è®ºï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹åªè¿”å› logitsï¼ˆé¢„æµ‹ç»“æœï¼‰ï¼Œä¸è¿”å› attentions\n",
      "\n",
      "============================================================\n",
      "ç¤ºä¾‹2ï¼šoutput_attentions = True\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¿”å›çš„å†…å®¹æœ‰è¿™äº›å±æ€§ï¼šodict_keys(['logits', 'past_key_values', 'attentions'])\n",
      "\n",
      "outputs.logits çš„å½¢çŠ¶: torch.Size([1, 3, 50257])\n",
      "outputs.attentions æ˜¯å¦ä¸º None: False\n",
      "\n",
      "âœ… ç°åœ¨å¯ä»¥çœ‹åˆ° attentions äº†ï¼\n",
      "   - GPT-2 æœ‰ 12 å±‚\n",
      "   - ç¬¬1å±‚çš„ attention å½¢çŠ¶: torch.Size([1, 12, 3, 3])\n",
      "     è§£é‡Šï¼š(batch_size=1, num_heads=12, seq_length=3, seq_length=3)\n",
      "   - è¿™æ„å‘³ç€ï¼š1ä¸ªæ ·æœ¬ï¼Œ12ä¸ªæ³¨æ„åŠ›å¤´ï¼Œ3ä¸ªtokenï¼Œæ¯ä¸ªtokenå¯¹å…¶ä»–3ä¸ªtokençš„æ³¨æ„åŠ›åˆ†æ•°\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "# ========== ç¤ºä¾‹1ï¼šoutput_attentions = Falseï¼ˆé»˜è®¤ï¼‰ ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"ç¤ºä¾‹1ï¼šoutput_attentions = Falseï¼ˆé»˜è®¤æƒ…å†µï¼‰\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# åŠ è½½æ¨¡å‹ï¼ˆé»˜è®¤é…ç½®ï¼‰\n",
    "model_default = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# å‡†å¤‡è¾“å…¥\n",
    "inputs = tokenizer(\"I love Python\", return_tensors=\"pt\")\n",
    "\n",
    "# è¿è¡Œæ¨¡å‹\n",
    "with torch.no_grad():\n",
    "    outputs_default = model_default(**inputs)\n",
    "\n",
    "print(f\"è¿”å›çš„å†…å®¹æœ‰è¿™äº›å±æ€§ï¼š{outputs_default.keys()}\")\n",
    "print(f\"\\noutputs.logits çš„å½¢çŠ¶: {outputs_default.logits.shape}\")\n",
    "print(f\"outputs.attentions æ˜¯å¦ä¸º None: {outputs_default.attentions is None}\")\n",
    "print(\"\\nç»“è®ºï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹åªè¿”å› logitsï¼ˆé¢„æµ‹ç»“æœï¼‰ï¼Œä¸è¿”å› attentions\\n\")\n",
    "\n",
    "\n",
    "# ========== ç¤ºä¾‹2ï¼šoutput_attentions = True ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"ç¤ºä¾‹2ï¼šoutput_attentions = True\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# åˆ›å»ºé…ç½®å¹¶è®¾ç½® output_attentions = True\n",
    "config = AutoConfig.from_pretrained(\"gpt2\")\n",
    "config.output_attentions = True\n",
    "\n",
    "# ä½¿ç”¨æ–°é…ç½®åŠ è½½æ¨¡å‹\n",
    "model_with_attn = AutoModelForCausalLM.from_pretrained(\"gpt2\", config=config)\n",
    "\n",
    "# è¿è¡Œæ¨¡å‹\n",
    "with torch.no_grad():\n",
    "    outputs_with_attn = model_with_attn(**inputs)\n",
    "\n",
    "print(f\"è¿”å›çš„å†…å®¹æœ‰è¿™äº›å±æ€§ï¼š{outputs_with_attn.keys()}\")\n",
    "print(f\"\\noutputs.logits çš„å½¢çŠ¶: {outputs_with_attn.logits.shape}\")\n",
    "print(f\"outputs.attentions æ˜¯å¦ä¸º None: {outputs_with_attn.attentions is None}\")\n",
    "\n",
    "if outputs_with_attn.attentions is not None:\n",
    "    print(f\"\\nâœ… ç°åœ¨å¯ä»¥çœ‹åˆ° attentions äº†ï¼\")\n",
    "    print(f\"   - GPT-2 æœ‰ {len(outputs_with_attn.attentions)} å±‚\")\n",
    "    print(f\"   - ç¬¬1å±‚çš„ attention å½¢çŠ¶: {outputs_with_attn.attentions[0].shape}\")\n",
    "    print(f\"     è§£é‡Šï¼š(batch_size=1, num_heads=12, seq_length=3, seq_length=3)\")\n",
    "    print(f\"   - è¿™æ„å‘³ç€ï¼š1ä¸ªæ ·æœ¬ï¼Œ12ä¸ªæ³¨æ„åŠ›å¤´ï¼Œ3ä¸ªtokenï¼Œæ¯ä¸ªtokenå¯¹å…¶ä»–3ä¸ªtokençš„æ³¨æ„åŠ›åˆ†æ•°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d062c",
   "metadata": {},
   "source": [
    "## å¯è§†åŒ–æ³¨æ„åŠ›åˆ†æ•°\n",
    "\n",
    "ä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹æ¨¡å‹åœ¨å¤„ç† \"I love Python\" æ—¶ï¼Œæ¯ä¸ªè¯å¯¹å…¶ä»–è¯çš„\"å…³æ³¨ç¨‹åº¦\"ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05520b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥å¥å­: 'I love Python'\n",
      "åˆ†è¯ç»“æœ: ['I', 'Ä love', 'Ä Python']\n",
      "\n",
      "============================================================\n",
      "ç¬¬1å±‚ã€ç¬¬1ä¸ªæ³¨æ„åŠ›å¤´çš„æ³¨æ„åŠ›åˆ†æ•°ï¼ˆæœªå½’ä¸€åŒ–ï¼‰\n",
      "============================================================\n",
      "å«ä¹‰ï¼šæ¯ä¸€è¡Œè¡¨ç¤ºè¯¥ token å¯¹æ‰€æœ‰ token çš„å…³æ³¨ç¨‹åº¦\n",
      "\n",
      "                     I       Ä love     Ä Python\n",
      "         I      1.0000      0.0000      0.0000\n",
      "     Ä love      0.8994      0.1006      0.0000\n",
      "   Ä Python      0.5894      0.1430      0.2675\n",
      "\n",
      "============================================================\n",
      "ä¸¾ä¾‹è¯´æ˜ï¼š\n",
      "============================================================\n",
      "å½“æ¨¡å‹å¤„ç† 'Ä love' è¿™ä¸ªè¯æ—¶ï¼š\n",
      "  - å¯¹ 'I' çš„å…³æ³¨åˆ†æ•°: 0.8994\n",
      "  - å¯¹ 'Ä love' çš„å…³æ³¨åˆ†æ•°: 0.1006\n",
      "  - å¯¹ 'Ä Python' çš„å…³æ³¨åˆ†æ•°: 0.0000\n",
      "\n",
      "åˆ†æ•°è¶Šå¤§ï¼Œè¡¨ç¤ºå…³æ³¨ç¨‹åº¦è¶Šé«˜ï¼\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "# ä½¿ç”¨ output_attentions = True çš„æ¨¡å‹\n",
    "config = AutoConfig.from_pretrained(\"gpt2\")\n",
    "config.output_attentions = True\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# è¾“å…¥å¥å­\n",
    "text = \"I love Python\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# è·å– tokensï¼ˆåˆ†è¯ç»“æœï¼‰\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "print(f\"è¾“å…¥å¥å­: '{text}'\")\n",
    "print(f\"åˆ†è¯ç»“æœ: {tokens}\\n\")\n",
    "\n",
    "# è¿è¡Œæ¨¡å‹\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# æå–ç¬¬1å±‚ã€ç¬¬1ä¸ªæ³¨æ„åŠ›å¤´çš„æ³¨æ„åŠ›åˆ†æ•°\n",
    "first_layer_first_head = outputs.attentions[0][0, 0]  # shape: (3, 3)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ç¬¬1å±‚ã€ç¬¬1ä¸ªæ³¨æ„åŠ›å¤´çš„æ³¨æ„åŠ›åˆ†æ•°ï¼ˆæœªå½’ä¸€åŒ–ï¼‰\")\n",
    "print(\"=\" * 60)\n",
    "print(\"å«ä¹‰ï¼šæ¯ä¸€è¡Œè¡¨ç¤ºè¯¥ token å¯¹æ‰€æœ‰ token çš„å…³æ³¨ç¨‹åº¦\\n\")\n",
    "\n",
    "# æ‰“å°æ³¨æ„åŠ›çŸ©é˜µ\n",
    "print(f\"{'':>10}\", end=\"\")\n",
    "for token in tokens:\n",
    "    print(f\"{token:>12}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"{token:>10}\", end=\"\")\n",
    "    for j in range(len(tokens)):\n",
    "        score = first_layer_first_head[i, j].item()\n",
    "        print(f\"{score:>12.4f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ä¸¾ä¾‹è¯´æ˜ï¼š\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"å½“æ¨¡å‹å¤„ç† '{tokens[1]}' è¿™ä¸ªè¯æ—¶ï¼š\")\n",
    "print(f\"  - å¯¹ '{tokens[0]}' çš„å…³æ³¨åˆ†æ•°: {first_layer_first_head[1, 0].item():.4f}\")\n",
    "print(f\"  - å¯¹ '{tokens[1]}' çš„å…³æ³¨åˆ†æ•°: {first_layer_first_head[1, 1].item():.4f}\")\n",
    "print(f\"  - å¯¹ '{tokens[2]}' çš„å…³æ³¨åˆ†æ•°: {first_layer_first_head[1, 2].item():.4f}\")\n",
    "print(\"\\nåˆ†æ•°è¶Šå¤§ï¼Œè¡¨ç¤ºå…³æ³¨ç¨‹åº¦è¶Šé«˜ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e77ebd4",
   "metadata": {},
   "source": [
    "## ğŸ“š æ€»ç»“\n",
    "\n",
    "### `config.output_attentions = True` çš„ä½œç”¨ï¼š\n",
    "\n",
    "1. **é»˜è®¤æƒ…å†µï¼ˆFalseï¼‰**ï¼š\n",
    "   - æ¨¡å‹åªè¿”å›æœ€ç»ˆé¢„æµ‹ç»“æœï¼ˆlogitsï¼‰\n",
    "   - å°±åƒè€å¸ˆåªå‘Šè¯‰ä½ ç­”æ¡ˆï¼Œä¸å‘Šè¯‰ä½ è§£é¢˜è¿‡ç¨‹\n",
    "   \n",
    "2. **è®¾ç½®ä¸º True**ï¼š\n",
    "   - æ¨¡å‹ä¼šé¢å¤–è¿”å›æ‰€æœ‰å±‚ã€æ‰€æœ‰æ³¨æ„åŠ›å¤´çš„æ³¨æ„åŠ›åˆ†æ•°\n",
    "   - å°±åƒè€å¸ˆä¸ä»…ç»™ä½ ç­”æ¡ˆï¼Œè¿˜è¯¦ç»†è®²è§£æ¯ä¸€æ­¥æ˜¯æ€ä¹ˆæƒ³çš„\n",
    "   \n",
    "3. **ä»€ä¹ˆæ—¶å€™éœ€è¦è®¾ç½®ä¸º Trueï¼Ÿ**\n",
    "   - âœ… æƒ³ç ”ç©¶æ¨¡å‹æ˜¯å¦‚ä½•\"ç†è§£\"å¥å­çš„\n",
    "   - âœ… æƒ³å¯è§†åŒ–æ¨¡å‹çš„æ³¨æ„åŠ›æ¨¡å¼\n",
    "   - âœ… æƒ³è°ƒè¯•æ¨¡å‹æˆ–åˆ†ææ¨¡å‹è¡Œä¸º\n",
    "   - âŒ åªæƒ³è¦é¢„æµ‹ç»“æœï¼ˆè®¾ç½®ä¸º True ä¼šå¢åŠ å†…å­˜æ¶ˆè€—å’Œè®¡ç®—æ—¶é—´ï¼‰\n",
    "\n",
    "### ç±»æ¯”ç†è§£ï¼š\n",
    "æƒ³è±¡ä½ åœ¨ç”¨ç¿»è¯‘è½¯ä»¶ç¿»è¯‘ \"I love Python\"ï¼š\n",
    "- **output_attentions = False**ï¼šåªç»™ä½ ç¿»è¯‘ç»“æœ \"æˆ‘çˆ± Python\"\n",
    "- **output_attentions = True**ï¼šç»™ä½ ç¿»è¯‘ç»“æœ + è§£é‡Šï¼ˆæ¯”å¦‚ç¿»è¯‘ \"love\" æ—¶ï¼Œè½¯ä»¶è€ƒè™‘äº†å‰é¢çš„ \"I\" å’Œåé¢çš„ \"Python\"ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### å®é™…åº”ç”¨åœºæ™¯ï¼š\n",
    "\n",
    "åœ¨æ‚¨çš„ `GroupedQueryAttention.ipynb` æ–‡ä»¶ä¸­ï¼š\n",
    "```python\n",
    "# éœ€è¦è®¾ç½® config.output_attentions = True\n",
    "config = AutoConfig.from_pretrained(\"gpt2\")\n",
    "config.output_attentions = True\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", config=config)\n",
    "\n",
    "# ç„¶åæ‰èƒ½è®¿é—® attentions\n",
    "outputs = gpt2(inputs.input_ids)\n",
    "first_layer_attentions = outputs.attentions[0][0]  # âœ… ä¸ä¼šæŠ¥é”™\n",
    "```\n",
    "\n",
    "å¦‚æœä¸è®¾ç½®ï¼Œ`outputs.attentions` ä¼šæ˜¯ä¸€ä¸ªåŒ…å« None å€¼çš„å…ƒç»„ï¼Œå¯¼è‡´ `TypeError: 'NoneType' object is not subscriptable` é”™è¯¯ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
